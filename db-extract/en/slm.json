{
    "word": "slm",
    "lang_code": "en",
    "source_file_to_entries": {
        "raw-wiktextract-data.jsonl": [
            {
                "entry_id": "b0490465-1977-3a9e-b36c-87234a282886",
                "word": "SLM",
                "pos": "name",
                "lang_code": "en",
                "forms": [],
                "senses": [
                    {
                        "sense_id": "69ae5220-803a-332c-b3df-eb9723689d7d",
                        "entry_id": "b0490465-1977-3a9e-b36c-87234a282886",
                        "glosses": [
                            "Initialism of Sudan Liberation Movement."
                        ],
                        "tags": [
                            "abbreviation",
                            "alt-of",
                            "initialism"
                        ],
                        "examples": []
                    }
                ],
                "translations": [],
                "word_linkages": [
                    {
                        "linkage_id": "f227b145-d013-4d8f-9843-81a49e9cbd3e",
                        "source_entry_id": "b0490465-1977-3a9e-b36c-87234a282886",
                        "source_sense_ids": [
                            "69ae5220-803a-332c-b3df-eb9723689d7d"
                        ],
                        "linkage_type": "synonyms",
                        "word": "SLA"
                    }
                ]
            },
            {
                "entry_id": "1d5ecf42-33fa-3e3d-bb00-367cfb4f9331",
                "word": "SLM",
                "pos": "noun",
                "lang_code": "en",
                "forms": [
                    {
                        "form_id": "ec3bc699-6da0-4230-bc30-c6abf32fdac9",
                        "entry_id": "1d5ecf42-33fa-3e3d-bb00-367cfb4f9331",
                        "tags": [
                            "plural"
                        ],
                        "form": "SLMs"
                    }
                ],
                "senses": [
                    {
                        "sense_id": "b3eeccc6-3e40-341e-8de9-416207b861a2",
                        "entry_id": "1d5ecf42-33fa-3e3d-bb00-367cfb4f9331",
                        "glosses": [
                            "Initialism of single–longitudinal-mode [laser] or single–longitudinal-mode laser."
                        ],
                        "tags": [
                            "abbreviation",
                            "alt-of",
                            "initialism"
                        ],
                        "examples": []
                    },
                    {
                        "sense_id": "02c28faa-10fa-381b-856c-421cc173f0bb",
                        "entry_id": "1d5ecf42-33fa-3e3d-bb00-367cfb4f9331",
                        "glosses": [
                            "Initialism of small language model."
                        ],
                        "tags": [
                            "abbreviation",
                            "alt-of",
                            "initialism"
                        ],
                        "examples": [
                            {
                                "example_id": "baf2ee5d-f333-47ff-add6-f5c37dfdc112",
                                "sense_id": "02c28faa-10fa-381b-856c-421cc173f0bb",
                                "text": "Large language models work well because they’re so large. The latest models from OpenAI, Meta, and DeepSeek use hundreds of billions of “parameters” […] With more parameters, the models are better able to identify patterns and connections, which in turn makes them more powerful and accurate. But this power comes at a cost […] huge computational resources […] energy hogs […] In response, some researchers are now thinking small. IBM, Google, Microsoft, and OpenAI have all recently released small language models (SLMs) that use a few billion parameters—a fraction of their LLM counterparts. Small models are not used as general-purpose tools like their larger cousins. But they can excel on specific, more narrowly defined tasks, such as summarizing conversations, answering patient questions as a health care chatbot, and gathering data in smart devices. “For a lot of tasks, an 8 billion–parameter model is actually pretty good,” said Zico Kolter, a computer scientist at Carnegie Mellon University. They can also run on a laptop or cell phone, instead of a huge data center. (There’s no consensus on the exact definition of “small,” but the new models all max out around 10 billion parameters.) To optimize the training process for these small models, researchers use a few tricks. […]"
                            }
                        ]
                    }
                ],
                "translations": [],
                "word_linkages": []
            }
        ],
        "ru-extract.jsonl": [],
        "nl-extract.jsonl": [],
        "pl-extract.jsonl": [],
        "fr-extract.jsonl": [],
        "it-extract.jsonl": [],
        "de-extract.jsonl": [],
        "cs-extract.jsonl": [],
        "es-extract.jsonl": [],
        "tr-extract.jsonl": [
            {
                "entry_id": "6894ffcf-62a1-3bbb-9ed1-3270918477d4",
                "word": "SLM",
                "pos": "abbrev",
                "lang_code": "en",
                "forms": [],
                "senses": [
                    {
                        "sense_id": "115cc22a-13cf-3f6f-9d99-10997c10f26d",
                        "entry_id": "6894ffcf-62a1-3bbb-9ed1-3270918477d4",
                        "glosses": [
                            "(havacılık) Service Lifecycle Management (Hizmet ömür yönetimi)"
                        ],
                        "tags": [],
                        "examples": []
                    }
                ],
                "translations": [],
                "word_linkages": []
            }
        ]
    }
}